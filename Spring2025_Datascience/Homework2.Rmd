---
title: "Homework2"
author: "Andrea Yang"
date: "2025-02-17"
output: html_document
---

## Data Processing

### Loading Package

```{r 1}
library(readxl)
library(dplyr)
library(stringr)
library(caret)
library(nnet)
library(randomForest)
library(xgboost)
library(e1071)
```

### Loading Data

```{r 2}
df <- read_excel("/Users/andreayyng/Downloads/BMIDS HW 2-1.xlsx", col_names = TRUE)
target_var <- names(df)[ncol(df)]
if (!is.factor(df[[target_var]])) {
  df[[target_var]] <- as.factor(df[[target_var]])
}
df
target_var
```

### Split Data

```{r 3}
set.seed(43) 
train_index <- createDataPartition(df[[target_var]], 
                                   p = 0.7, 
                                   list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# used chatgpt as to format 
cat("Training samples:", nrow(train_data), 
    "\nTest samples:", nrow(test_data),
    "\nClass distribution:\n")
prop.table(table(df[[target_var]])) 
prop.table(table(train_data[[target_var]])) 
```

### Model Training

#### Random Forest model

```{r 4}
set.seed(116)
rf_model <- randomForest(reformulate(".", response = target_var),
                         data = train_data,
                         ntree = 500)
rf_pred <- predict(rf_model, test_data)
rf_cm <- confusionMatrix(rf_pred, test_data[[target_var]])


```

#### XGBoost model

```{r 5}
train_matrix <- model.matrix(reformulate(".", response = target_var), 
                          train_data)[,-1]
test_matrix <- model.matrix(reformulate(".", response = target_var),
                         test_data)[,-1]

xgb_model <- xgboost(
  data = train_matrix,
  label = as.integer(train_data[[target_var]]) - 1,
  max_depth = 6,
  eta = 0.3,
  nrounds = 100,
  objective = "multi:softmax",
  num_class = length(levels(train_data[[target_var]])),
  verbose = 0
)
xgb_pred <- factor(levels(train_data[[target_var]])[predict(xgb_model, test_matrix) + 1],
                 levels = levels(train_data[[target_var]]))
xgb_cm <- confusionMatrix(xgb_pred, test_data[[target_var]])
```

#### SVM model

```{r 6}
svm_model <- svm(reformulate(".", response = target_var),
                data = train_data,
                kernel = "radial")
svm_pred <- predict(svm_model, test_data)
svm_cm <- confusionMatrix(svm_pred, test_data[[target_var]])

```

### Performance

```{r 7}
#use chatgpt to format my comparison result 
results <- data.frame(
  Model = c("Random Forest", "XGBoost", "SVM"),
  Accuracy = c(rf_cm$overall["Accuracy"],
              xgb_cm$overall["Accuracy"],
              svm_cm$overall["Accuracy"])
)

print("=== Model Comparison ===")
print(results)

print("\n=== Detailed Confusion Matrices ===")

print("Random Forest:")
print(rf_cm$table)
cat("Accuracy:", rf_cm$overall["Accuracy"], "\n\n")

print("XGBoost:")
print(xgb_cm$table)
cat("Accuracy:", xgb_cm$overall["Accuracy"], "\n\n")

print("SVM:")
print(svm_cm$table)
cat("Accuracy:", svm_cm$overall["Accuracy"], "\n")

metric_table <- data.frame(
  Model = rep(c("RF", "XGB", "SVM"), each = 3),
  Metric = rep(c("Precision", "Recall", "F1"), 3),
  Value = c(rf_cm$byClass[c("Precision", "Recall", "F1")],
           xgb_cm$byClass[c("Precision", "Recall", "F1")],
           svm_cm$byClass[c("Precision", "Recall", "F1")])
)
metric_table
```

## Discussion

Among these models, XGBoost performed better compared to the other two models. While XGBoost and Random Forest both achieved more than 98% overall accuracy, SVM only reached 97.68%, making it the worst performer. Random Forest had 2 false negatives for LUAD and 3 false positives for LUSC, with precision of 0.9834, recall of 0.9889, and F1 score of 0.9861. XGBoost had 4 false positives but no false negatives, achieving precision of 0.9783, recall of 1.0000, and F1 score of 0.9890. This gave XGBoost 100% LUAD recall compared to Random Forest's 98.89%. Though XGBoost had slightly lower LUSC recall (97.58%) than Random Forest (98.18%), this trade-off is acceptable. Hence, XGBoost is considered as the best model for this data.
